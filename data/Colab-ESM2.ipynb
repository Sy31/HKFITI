{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "J0urkyB6sTEZ",
    "outputId": "55ae38e4-edce-44fa-b921-19660472fb7b",
    "executionInfo": {
     "status": "error",
     "timestamp": 1763464619655,
     "user_tz": -480,
     "elapsed": 5337,
     "user": {
      "displayName": "21 Shiryu",
      "userId": "15963607702740956570"
     }
    }
   },
   "source": [
    "\"\"\"\n",
    "ESM-2 650M Protein Embedding Tool – Two-column Fast Format\n",
    "Designed for Google Colab\n",
    "\"\"\"\n",
    "\n",
    "#@title 1. Install required packages\n",
    "!pip install fair-esm biopython -q\n",
    "print(\"Packages installed successfully.\")\n",
    "\n",
    "#@title 2. Import libraries\n",
    "import torch\n",
    "import esm\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries loaded successfully.\")\n",
    "\n",
    "#@title 3. Load ESM-2 650M model\n",
    "print(\"Loading ESM-2 650M model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "print(\"ESM-2 650M model loaded.\")\n",
    "\n",
    "#@title 4. Define helper functions\n",
    "def parse_protein_file(file_path: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Parse protein file.\n",
    "    Supported format:\n",
    "        - Four-column format: index<TAB>UniProt<TAB>Name<TAB>Sequence\n",
    "    Returns:\n",
    "        List of (index, protein_id, sequence)\n",
    "    \"\"\"\n",
    "    proteins = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Skip header if detected\n",
    "    start_idx = 0\n",
    "    if lines and lines[0].strip():\n",
    "        first_parts = lines[0].strip().split('\\t')\n",
    "        if len(first_parts) >= 2:\n",
    "            try:\n",
    "                int(first_parts[0])  # header detection\n",
    "            except ValueError:\n",
    "                print(f\"Skipping header: {lines[0].strip()[:100]}\")\n",
    "                start_idx = 1\n",
    "\n",
    "    # Parse entries\n",
    "    for i, line in enumerate(lines[start_idx:], 1):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            if len(parts) == 4:  # expected four columns\n",
    "                idx = parts[0]\n",
    "                protein_id = parts[1]\n",
    "                sequence = parts[3]\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line[:50]}\")\n",
    "                continue\n",
    "\n",
    "            # Clean sequence (keep standard amino acids only)\n",
    "            sequence = re.sub(r'[^ACDEFGHIKLMNPQRSTVWY]', '', sequence.upper())\n",
    "\n",
    "            if sequence:\n",
    "                proteins.append((idx, protein_id, sequence))\n",
    "\n",
    "    return proteins\n",
    "\n",
    "def get_esm_embeddings(sequences: List[Tuple[str, str]], batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Generate ESM embeddings for given sequences.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "\n",
    "    for i in tqdm(range(0, len(sequences), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = sequences[i:i + batch_size]\n",
    "\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(\n",
    "            [(label, seq) for label, seq in batch]\n",
    "        )\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "            token_representations = results[\"representations\"][33]\n",
    "\n",
    "            for j, (label, seq) in enumerate(batch):\n",
    "                seq_embedding = token_representations[j, 1:len(seq)+1].mean(0)\n",
    "                embeddings[label] = seq_embedding.cpu().numpy()\n",
    "\n",
    "        del batch_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def save_embeddings(embeddings: dict, output_dir: str = \"protein_embeddings\"):\n",
    "    \"\"\"\n",
    "    Save embeddings as .npy files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save each protein embedding\n",
    "    for protein_id, embedding in embeddings.items():\n",
    "        file_path = os.path.join(output_dir, f\"{protein_id}.npy\")\n",
    "        np.save(file_path, embedding)\n",
    "\n",
    "    # Save all embeddings\n",
    "    all_embeddings = np.stack(list(embeddings.values()))\n",
    "    all_ids = list(embeddings.keys())\n",
    "\n",
    "    np.save(os.path.join(output_dir, \"all_embeddings.npy\"), all_embeddings)\n",
    "\n",
    "    with open(os.path.join(output_dir, \"protein_ids.txt\"), 'w') as f:\n",
    "        for pid in all_ids:\n",
    "            f.write(f\"{pid}\\n\")\n",
    "\n",
    "    return output_dir\n",
    "\n",
    "#@title 5. Main workflow\n",
    "\n",
    "def process_protein_file(file_path: str, batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Process the uploaded protein file and generate embeddings.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing file: {file_path}\")\n",
    "\n",
    "    # 1. Parse file\n",
    "    print(\"Parsing protein sequences...\")\n",
    "    proteins = parse_protein_file(file_path)\n",
    "\n",
    "    if not proteins:\n",
    "        print(\"No valid protein sequences found.\")\n",
    "        return None, {}\n",
    "\n",
    "    print(f\"Found {len(proteins)} protein sequences.\")\n",
    "\n",
    "    # Prepare sequences\n",
    "    sequences = [(protein_id, seq) for idx, protein_id, seq in proteins]\n",
    "\n",
    "    # Preview\n",
    "    print(\"\\nSequence samples:\")\n",
    "    for idx, protein_id, seq in proteins[:5]:\n",
    "        preview = seq[:30] + \"...\" if len(seq) > 30 else seq\n",
    "        print(f\"  ID: {idx} | Length: {len(seq)} | Sequence: {preview}\")\n",
    "    if len(proteins) > 5:\n",
    "        print(f\"  ... {len(proteins)-5} more sequences\")\n",
    "\n",
    "    # 2. Generate embeddings\n",
    "    print(f\"\\nGenerating embeddings (batch size: {batch_size})...\")\n",
    "    embeddings = get_esm_embeddings(sequences, batch_size)\n",
    "\n",
    "    # 3. Save embeddings\n",
    "    print(\"\\nSaving embeddings...\")\n",
    "    output_dir = save_embeddings(embeddings)\n",
    "\n",
    "    # 4. Zip output\n",
    "    print(\"\\nCreating archive...\")\n",
    "    zip_filename = \"protein_embeddings.zip\"\n",
    "    !zip -r {zip_filename} {output_dir} -q\n",
    "\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Embedding dimension: {list(embeddings.values())[0].shape}\")\n",
    "    print(f\"Output directory: {output_dir}/\")\n",
    "\n",
    "    return zip_filename, embeddings\n",
    "\n",
    "#@title 6. Upload and process file\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ESM-2 650M Protein Embedding Tool\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFile requirements:\")\n",
    "print(\"• Four-column format: index<TAB>UniProt<TAB>Name<TAB>Sequence\")\n",
    "print(\"\\nExamples:\")\n",
    "print(\"1\\tP12345\\tAlpha\\tMKLLIALSLGALV...\")\n",
    "print(\"2\\tP67890\\tBeta\\tMASNFTQFVLVDNG...\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nUpload your protein sequence file (TXT):\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    print(f\"\\nFile uploaded: {filename}\")\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    try:\n",
    "        zip_file, embeddings = process_protein_file(filename, batch_size)\n",
    "\n",
    "        if zip_file:\n",
    "            print(\"\\nPreparing download...\")\n",
    "            files.download(zip_file)\n",
    "            print(\"Download started.\")\n",
    "\n",
    "            print(\"\\nSummary:\")\n",
    "            print(f\"  Proteins processed: {len(embeddings)}\")\n",
    "            print(f\"  Embedding dimension: {list(embeddings.values())[0].shape[0]}\")\n",
    "            print(\"  Output files:\")\n",
    "            print(\"    • all_embeddings.npy\")\n",
    "            print(\"    • protein_ids.txt\")\n",
    "            print(\"    • Individual .npy files per protein\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError encountered: {str(e)}\")\n",
    "        print(\"Please check input file format.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nNo file uploaded.\")\n",
    "\n",
    "#@title 7. Optional: View embedding sample\n",
    "if 'embeddings' in locals() and embeddings:\n",
    "    print(\"\\nEmbedding sample:\")\n",
    "    first_key = list(embeddings.keys())[0]\n",
    "    first_embedding = embeddings[first_key]\n",
    "    print(f\"Protein ID: {first_key}\")\n",
    "    print(f\"Embedding shape: {first_embedding.shape}\")\n",
    "    print(f\"First 10 values: {first_embedding[:10]}\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(first_embedding, bins=50, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Value Distribution (ID: {first_key})')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # First 100 dimensions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(first_embedding[:100], alpha=0.7)\n",
    "    plt.title('First 100 Dimensions')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": [],
   "machine_shape": "hm",
   "authorship_tag": "ABX9TyN1k98p2mBANeu/H3OcHLQe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
