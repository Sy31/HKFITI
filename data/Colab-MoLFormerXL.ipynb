{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A2VJlmtfdrRp",
    "outputId": "26b72716-882f-44e0-ab56-509f2fdaf411",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761209465258,
     "user_tz": -480,
     "elapsed": 49741,
     "user": {
      "displayName": "21 Shiryu",
      "userId": "15963607702740956570"
     }
    }
   },
   "source": [
    "# MoLFormer-XL Enhanced Version â€“ Compatible with Two-Column Input Files\n",
    "\n",
    "# Install required packages\n",
    "!pip install transformers torch numpy pandas tqdm -q\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def load_molformer_model(model_variant=\"both-10pct\"):\n",
    "    \"\"\"\n",
    "    Load the MoLFormer-XL model.\n",
    "    model_variant: \n",
    "        - 'both-10pct' (recommended)\n",
    "        - 'both-1pct' (faster, lower accuracy)\n",
    "    \"\"\"\n",
    "    model_name = f\"ibm/MoLFormer-XL-{model_variant}\"\n",
    "    print(f\"Loading model: {model_name}...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(model_name, deterministic_eval=True, trust_remote_code=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"Using CPU (processing may be slow)\")\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def parse_smiles_file(content):\n",
    "    \"\"\"\n",
    "    Parse a SMILES file and return a list of entries.\n",
    "    Supported formats:\n",
    "        1. Two columns: index<TAB>SMILES (recommended)\n",
    "        2. One column: SMILES only\n",
    "        3. Three columns: id<TAB>db_id<TAB>SMILES (legacy format)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    lines = content.strip().split('\\n')\n",
    "\n",
    "    # Detect and skip header if present\n",
    "    first_line = lines[0].split('\\t')\n",
    "    start_idx = 0\n",
    "\n",
    "    if len(first_line) >= 2:\n",
    "        try:\n",
    "            int(first_line[0])  # attempt numeric index\n",
    "        except ValueError:\n",
    "            print(f\"Skipping header: {lines[0][:100]}...\")\n",
    "            start_idx = 1\n",
    "\n",
    "    # Parse content\n",
    "    for i, line in enumerate(lines[start_idx:], 1):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        parts = line.split('\\t')\n",
    "\n",
    "        if len(parts) >= 3:  # legacy three-column format\n",
    "            data.append({\n",
    "                'id': parts[0],\n",
    "                'db_id': parts[1],\n",
    "                'smiles': parts[2].strip()\n",
    "            })\n",
    "        elif len(parts) == 2:  # two-column format\n",
    "            data.append({\n",
    "                'id': parts[0].strip(),\n",
    "                'db_id': f'MOL_{parts[0].strip()}',\n",
    "                'smiles': parts[1].strip()\n",
    "            })\n",
    "        elif len(parts) == 1:  # one-column SMILES-only format\n",
    "            data.append({\n",
    "                'id': str(i),\n",
    "                'db_id': f'MOL_{i}',\n",
    "                'smiles': parts[0].strip()\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Skipping malformed line {i+start_idx}: {line[:50]}...\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def smiles_to_embeddings(smiles_list, model, tokenizer, device, batch_size=16):\n",
    "    \"\"\"Convert a batch of SMILES strings into embeddings.\"\"\"\n",
    "    vectors = []\n",
    "    failed_indices = []\n",
    "\n",
    "    # Adjust batch size based on available hardware\n",
    "    if torch.cuda.is_available():\n",
    "        batch_size = min(batch_size, 16)\n",
    "    else:\n",
    "        batch_size = min(batch_size, 4)\n",
    "\n",
    "    for i in tqdm(range(0, len(smiles_list), batch_size), desc=\"Processing\"):\n",
    "        batch = smiles_list[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=202\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                    embeddings = outputs.pooler_output\n",
    "                else:\n",
    "                    hidden_states = outputs.last_hidden_state\n",
    "                    embeddings = hidden_states[:, 0, :]  # CLS token\n",
    "\n",
    "            vectors.extend(embeddings.cpu().numpy())\n",
    "\n",
    "        except Exception:\n",
    "            print(f\"\\nBatch {i//batch_size + 1} failed. Processing entries individually...\")\n",
    "\n",
    "            for j, smiles in enumerate(batch):\n",
    "                try:\n",
    "                    inputs = tokenizer(\n",
    "                        [smiles],\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=202\n",
    "                    )\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                            embedding = outputs.pooler_output[0]\n",
    "                        else:\n",
    "                            embedding = outputs.last_hidden_state[0, 0, :]\n",
    "\n",
    "                    vectors.append(embedding.cpu().numpy())\n",
    "\n",
    "                except Exception:\n",
    "                    print(f\"  Skipping invalid SMILES (index {i+j}): {smiles[:30]}...\")\n",
    "                    vectors.append(np.zeros(768))\n",
    "                    failed_indices.append(i+j)\n",
    "\n",
    "        if torch.cuda.is_available() and i % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    return np.array(vectors), failed_indices\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main program.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\" MoLFormer-XL SMILES Embedding Tool \")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"\\nSupported file formats:\")\n",
    "    print(\"1. Two-column (tab-separated): index<TAB>SMILES\")\n",
    "    print(\"2. One-column: SMILES only\")\n",
    "    print(\"3. Three-column: id<TAB>db_id<TAB>SMILES\")\n",
    "    print(\"\\nHeaders are automatically detected and skipped.\")\n",
    "\n",
    "    print(\"\\nSelect model variant:\")\n",
    "    print(\"1. MoLFormer-XL-both-10pct (recommended)\")\n",
    "    print(\"2. MoLFormer-XL-both-1pct (faster)\")\n",
    "\n",
    "    choice = input(\"Select (1 or 2, default 1): \").strip() or '1'\n",
    "    variant = \"both-10pct\" if choice == '1' else \"both-1pct\"\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer, device = load_molformer_model(variant)\n",
    "\n",
    "    print(\"\\nUpload SMILES file...\")\n",
    "    print(\"Examples:\")\n",
    "    print(\"1\\tCCCCCC\")\n",
    "    print(\"2\\tCOc1ccccc1\")\n",
    "    print(\"3\\tCCN(CC)CC\")\n",
    "\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    if not uploaded:\n",
    "        print(\"No file uploaded.\")\n",
    "        return\n",
    "\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    content = uploaded[filename].decode('utf-8')\n",
    "\n",
    "    print(\"\\nParsing file...\")\n",
    "    data = parse_smiles_file(content)\n",
    "\n",
    "    if not data:\n",
    "        print(\"Error: No valid SMILES found. Check file format.\")\n",
    "        return\n",
    "\n",
    "    smiles_list = [item['smiles'] for item in data]\n",
    "\n",
    "    print(f\"\\nParsed {len(smiles_list)} SMILES entries.\")\n",
    "    print(\"\\nFirst 5 entries:\")\n",
    "    for i, item in enumerate(data[:5], 1):\n",
    "        preview = item['smiles'][:50] + \"...\" if len(item['smiles']) > 50 else item['smiles']\n",
    "        print(f\"{i}. ID={item['id']}, SMILES={preview}\")\n",
    "\n",
    "    if len(smiles_list) > 100:\n",
    "        cont = input(f\"\\nProcessing {len(smiles_list)} entries may take time. Continue? (y/n, default y): \").strip().lower()\n",
    "        if cont == 'n':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "\n",
    "    print(\"\\nStarting embedding generation...\")\n",
    "    vectors, failed_indices = smiles_to_embeddings(smiles_list, model, tokenizer, device)\n",
    "\n",
    "    if failed_indices:\n",
    "        print(f\"\\nWarning: {len(failed_indices)} entries failed.\")\n",
    "        print(\"Sample failed IDs:\", [data[idx]['id'] for idx in failed_indices[:10]],\n",
    "              \"...\" if len(failed_indices) > 10 else \"\")\n",
    "\n",
    "    print(\"\\nEmbedding generation completed.\")\n",
    "    print(f\"Array shape: {vectors.shape}\")\n",
    "    print(f\"Embedding dimension: {vectors.shape[1]}\")\n",
    "    print(f\"Memory usage: {vectors.nbytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "    norms = np.linalg.norm(vectors, axis=1)\n",
    "    print(\"\\nVector statistics:\")\n",
    "    print(f\"- Mean norm: {np.mean(norms):.4f}\")\n",
    "    print(f\"- Std norm: {np.std(norms):.4f}\")\n",
    "    print(f\"- Min norm: {np.min(norms):.4f}\")\n",
    "    print(f\"- Max norm: {np.max(norms):.4f}\")\n",
    "\n",
    "    zero_vectors = np.sum(norms < 0.01)\n",
    "    if zero_vectors > 0:\n",
    "        print(f\"- Zero vectors: {zero_vectors} (likely from failed SMILES)\")\n",
    "\n",
    "    print(\"\\nSaving output files...\")\n",
    "\n",
    "    vector_file = 'smiles_vectors_molformer.npy'\n",
    "    np.save(vector_file, vectors)\n",
    "\n",
    "    full_data = {\n",
    "        'vectors': vectors,\n",
    "        'ids': [item['id'] for item in data],\n",
    "        'db_ids': [item['db_id'] for item in data],\n",
    "        'smiles': smiles_list,\n",
    "        'failed_indices': failed_indices,\n",
    "        'model': f'MoLFormer-XL-{variant}',\n",
    "        'dimension': vectors.shape[1]\n",
    "    }\n",
    "\n",
    "    full_file = 'molformer_xl_data.npz'\n",
    "    np.savez_compressed(full_file, **full_data)\n",
    "\n",
    "    csv_file = 'molformer_xl_mapping.csv'\n",
    "    df = pd.DataFrame({\n",
    "        'ID': [item['id'] for item in data],\n",
    "        'SMILES': smiles_list,\n",
    "        'Vector_Index': list(range(len(data))),\n",
    "        'Status': ['Failed' if i in failed_indices else 'OK' for i in range(len(data))]\n",
    "    })\n",
    "    df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(\"\\nDownloading files...\")\n",
    "    files.download(vector_file)\n",
    "    files.download(full_file)\n",
    "    files.download(csv_file)\n",
    "\n",
    "    print(\"\\nFiles generated:\")\n",
    "    print(f\"1. {vector_file}: Embedding matrix\")\n",
    "    print(f\"2. {full_file}: Full dataset with metadata\")\n",
    "    print(f\"3. {csv_file}: Mapping file\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" Done. Files downloaded. \")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return vectors\n",
    "\n",
    "# Run program\n",
    "if __name__ == \"__main__\":\n",
    "    vectors = main()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "StqFsTXpsOpZ"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1cX90mOQpxv2F31uCK-qpJV-19qgXYyHE",
     "timestamp": 1758511900213
    }
   ],
   "gpuType": "A100",
   "authorship_tag": "ABX9TyOdzgeLBuj2cgGDvDHNLWTx"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
